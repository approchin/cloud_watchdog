#!/usr/bin/env python3
"""
LangGraph Agent æµ‹è¯•å¥—ä»¶

åŒ…å«ï¼š
1. ç¦»çº¿æµ‹è¯•ï¼šä¸è°ƒç”¨çœŸå® APIï¼Œæµ‹è¯•é€»è¾‘å’Œæ•°æ®æµ
2. åœ¨çº¿æµ‹è¯•ï¼šè°ƒç”¨çœŸå® DeepSeek APIï¼ŒéªŒè¯ç«¯åˆ°ç«¯åŠŸèƒ½
3. é›†æˆæµ‹è¯•ï¼šæ¨¡æ‹Ÿå®Œæ•´ç›‘æ§æµç¨‹

è¿è¡Œæ–¹å¼ï¼š
    # ä»…ç¦»çº¿æµ‹è¯•ï¼ˆå¿«é€Ÿï¼‰
    python tests/test_agent.py --offline
    
    # ä»…åœ¨çº¿æµ‹è¯•ï¼ˆéœ€è¦ API Keyï¼‰
    python tests/test_agent.py --online
    
    # å…¨éƒ¨æµ‹è¯•
    python tests/test_agent.py
"""

import sys
import json
import time
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from watchdog.agent import (
    analyze_with_llm,
    DiagnosisAgent,
    run_diagnosis,
    SYSTEM_PROMPT
)
from watchdog.config import init_config


# ============================================
# æµ‹è¯•æ•°æ®
# ============================================

def get_mock_evidence_cpu_high() -> Dict[str, Any]:
    """æ¨¡æ‹Ÿ CPU é«˜è´Ÿè½½çš„ evidence"""
    return {
        "event_id": "evt_test_001",
        "timestamp": datetime.now().isoformat(),
        "container": {
            "id": "abc123",
            "name": "test-cpu-stress",
            "image": "test/cpu-stress:latest",
            "status": "running",
            "running": True,
            "restarting": False,
            "paused": False,
            "oom_killed": False,
            "exit_code": 0,
            "error": "",
            "started_at": "2025-12-11T10:00:00.000000Z",
            "finished_at": "0001-01-01T00:00:00Z",
            "restart_count": 0,
            "restart_policy": "always",
            "memory_limit": 0,
            "cpu_limit": 0,
            "ip_address": "172.17.0.2",
            "ports": {}
        },
        "evidence": {
            "exit_code": 0,
            "oom_killed": False,
            "error_message": "",
            "cpu_percent": "95.2%",
            "memory_percent": "45.0%",
            "memory_usage": "230MiB / 512MiB",
            "logs_tail": "Running stress test...",
            "restart_count_24h": 0,
            "health_check": {
                "healthy": True,
                "message": "ok"
            }
        },
        "fault_type": "CPU_HIGH",
        "thresholds": {
            "cpu_warning": 70,
            "cpu_critical": 80,
            "memory_warning": 70,
            "memory_critical": 85
        }
    }


def get_mock_evidence_normal() -> Dict[str, Any]:
    """æ¨¡æ‹Ÿæ­£å¸¸è¿è¡Œçš„ evidence"""
    return {
        "event_id": "evt_test_002",
        "timestamp": datetime.now().isoformat(),
        "container": {
            "id": "xyz789",
            "name": "test-normal-app",
            "image": "nginx:latest",
            "status": "running",
            "running": True,
            "restarting": False,
            "paused": False,
            "oom_killed": False,
            "exit_code": 0,
            "error": "",
            "started_at": "2025-12-10T00:00:00.000000Z",
            "finished_at": "0001-01-01T00:00:00Z",
            "restart_count": 0,
            "restart_policy": "always",
            "memory_limit": 536870912,
            "cpu_limit": 0,
            "ip_address": "172.17.0.3",
            "ports": {}
        },
        "evidence": {
            "exit_code": 0,
            "oom_killed": False,
            "error_message": "",
            "cpu_percent": "5.0%",
            "memory_percent": "20.0%",
            "memory_usage": "102MiB / 512MiB",
            "logs_tail": "GET /index.html 200",
            "restart_count_24h": 0,
            "health_check": {
                "healthy": True,
                "message": "HTTP 200"
            }
        },
        "fault_type": "UNKNOWN",
        "thresholds": {
            "cpu_warning": 70,
            "cpu_critical": 90,
            "memory_warning": 70,
            "memory_critical": 85
        }
    }


def get_mock_evidence_crash() -> Dict[str, Any]:
    """æ¨¡æ‹Ÿå®¹å™¨å´©æºƒçš„ evidence"""
    return {
        "event_id": "evt_test_003",
        "timestamp": datetime.now().isoformat(),
        "container": {
            "id": "def456",
            "name": "test-crash-loop",
            "image": "test/crash:latest",
            "status": "exited",
            "running": False,
            "restarting": False,
            "paused": False,
            "oom_killed": False,
            "exit_code": 1,
            "error": "",
            "started_at": "2025-12-11T10:00:00.000000Z",
            "finished_at": "2025-12-11T10:01:00.000000Z",
            "restart_count": 5,
            "restart_policy": "always",
            "memory_limit": 0,
            "cpu_limit": 0,
            "ip_address": "",
            "ports": {}
        },
        "evidence": {
            "exit_code": 1,
            "oom_killed": False,
            "error_message": "Application crashed",
            "cpu_percent": "0%",
            "memory_percent": "0%",
            "memory_usage": "0MiB / 512MiB",
            "logs_tail": "Error: Segmentation fault",
            "restart_count_24h": 5,
            "health_check": {
                "healthy": False,
                "message": "Container not running"
            }
        },
        "fault_type": "PROCESS_CRASH",
        "thresholds": {
            "cpu_warning": 70,
            "cpu_critical": 90,
            "memory_warning": 70,
            "memory_critical": 85
        }
    }


# ============================================
# ç¦»çº¿æµ‹è¯•ï¼ˆä¸éœ€è¦ API Keyï¼‰
# ============================================

def test_offline_prompt_generation():
    """æµ‹è¯• SYSTEM_PROMPT ç”Ÿæˆ"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šSYSTEM_PROMPT ç”Ÿæˆ")
    print("="*80)
    
    evidence = get_mock_evidence_cpu_high()
    evidence_str = json.dumps(evidence, ensure_ascii=False, indent=2)
    
    prompt = SYSTEM_PROMPT.format(evidence_str=evidence_str)
    
    # éªŒè¯ prompt åŒ…å«å…³é”®å†…å®¹
    assert "å®¹å™¨æ•…éšœè¯Šæ–­ä¸“å®¶" in prompt
    assert "test-cpu-stress" in prompt
    assert "95.2%" in prompt
    assert "CPU_HIGH" in prompt
    
    print("âœ… SYSTEM_PROMPT ç”Ÿæˆæ­£ç¡®")
    print(f"   - Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"   - åŒ…å« evidence: âœ“")
    print(f"   - åŒ…å«å†³ç­–è§„åˆ™: âœ“")
    print(f"   - åŒ…å« few-shot ç¤ºä¾‹: âœ“")
    
    return True


def test_offline_decision_validation():
    """æµ‹è¯•å†³ç­–æ ¼å¼éªŒè¯"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šå†³ç­–æ ¼å¼éªŒè¯")
    print("="*80)
    
    # æ¨¡æ‹Ÿ LLM è¿”å›çš„å†³ç­–
    valid_decision = {
        "fault_type": "CPU_HIGH",
        "command": "ALERT_ONLY",
        "params": {
            "container_name": "test-cpu-stress",
            "current_cpu": "95.2%",
            "current_memory": "45.0%",
            "retry_count": 0
        },
        "reason": "CPU ä½¿ç”¨ç‡è¿‡é«˜"
    }
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['fault_type', 'command', 'params', 'reason']
    for field in required_fields:
        assert field in valid_decision, f"ç¼ºå°‘å­—æ®µ: {field}"
    
    # éªŒè¯ params ä¸­çš„ container_name
    assert 'container_name' in valid_decision['params']
    
    # éªŒè¯ command æšä¸¾
    valid_commands = ['RESTART', 'STOP', 'ALERT_ONLY', 'NONE']
    assert valid_decision['command'] in valid_commands
    
    print("âœ… å†³ç­–æ ¼å¼éªŒè¯é€šè¿‡")
    print(f"   - å¿…éœ€å­—æ®µ: {required_fields}")
    print(f"   - command: {valid_decision['command']}")
    print(f"   - container_name: {valid_decision['params']['container_name']}")
    
    return True


def test_offline_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†é€»è¾‘"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šé”™è¯¯å¤„ç†")
    print("="*80)
    
    # æµ‹è¯•ç¼ºå°‘ API Key çš„æƒ…å†µ
    os.environ.pop('DEEPSEEK_API_KEY', None)  # ç¡®ä¿æ²¡æœ‰ API Key
    
    # é‡æ–°åˆå§‹åŒ–é…ç½®
    init_config()
    
    evidence = get_mock_evidence_normal()
    decision = analyze_with_llm(evidence)
    
    # åº”è¯¥è¿”å›é”™è¯¯å†³ç­–
    assert decision['fault_type'] == 'CONFIG_ERROR'
    assert decision['command'] == 'ALERT_ONLY'
    assert 'API Key' in decision['reason']
    
    print("âœ… é”™è¯¯å¤„ç†æ­£ç¡®")
    print(f"   - fault_type: {decision['fault_type']}")
    print(f"   - command: {decision['command']}")
    print(f"   - reason: {decision['reason'][:50]}...")
    
    return True


# ============================================
# åœ¨çº¿æµ‹è¯•ï¼ˆéœ€è¦ DeepSeek API Keyï¼‰
# ============================================

def test_online_llm_call():
    """æµ‹è¯•çœŸå®çš„ LLM è°ƒç”¨"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šDeepSeek API è°ƒç”¨ï¼ˆåœ¨çº¿ï¼‰")
    print("="*80)
    
    # æ£€æŸ¥ API Key
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âš ï¸  è·³è¿‡ï¼šæœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        return False
    
    # é‡æ–°åˆå§‹åŒ–é…ç½®
    init_config()
    
    evidence = get_mock_evidence_cpu_high()
    
    print(f"è°ƒç”¨ DeepSeek åˆ†æå®¹å™¨: {evidence['container']['name']}")
    start_time = time.time()
    
    decision = analyze_with_llm(evidence)
    
    elapsed = time.time() - start_time
    
    # éªŒè¯è¿”å›ç»“æœ
    assert 'fault_type' in decision
    assert 'command' in decision
    assert 'params' in decision
    assert 'reason' in decision
    
    print(f"âœ… API è°ƒç”¨æˆåŠŸï¼ˆè€—æ—¶: {elapsed:.2f}ç§’ï¼‰")
    print(f"   - fault_type: {decision['fault_type']}")
    print(f"   - command: {decision['command']}")
    print(f"   - reason: {decision['reason'][:80]}...")
    
    # ä¿å­˜ç»“æœ
    result_file = Path(__file__).parent.parent / "logs" / "test_agent_online_result.json"
    result_file.parent.mkdir(exist_ok=True)
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": datetime.now().isoformat(),
            "evidence": evidence,
            "decision": decision,
            "elapsed_seconds": elapsed
        }, f, ensure_ascii=False, indent=2)
    
    print(f"   - ç»“æœå·²ä¿å­˜: {result_file}")
    
    return True


def test_online_multiple_scenarios():
    """æµ‹è¯•å¤šç§åœºæ™¯çš„å†³ç­–å‡†ç¡®æ€§"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šå¤šåœºæ™¯å†³ç­–å‡†ç¡®æ€§ï¼ˆåœ¨çº¿ï¼‰")
    print("="*80)
    
    # æ£€æŸ¥ API Key
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âš ï¸  è·³è¿‡ï¼šæœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        return False
    
    init_config()
    
    test_cases = [
        (get_mock_evidence_cpu_high(), "ALERT_ONLY", "CPU é«˜è´Ÿè½½åº”è¯¥ä»…å‘Šè­¦"),
        (get_mock_evidence_normal(), "NONE", "æ­£å¸¸å®¹å™¨ä¸éœ€è¦æ“ä½œ"),
        (get_mock_evidence_crash(), "RESTART", "å´©æºƒå®¹å™¨åº”è¯¥é‡å¯"),
    ]
    
    results = []
    
    for evidence, expected_command, description in test_cases:
        container_name = evidence['container']['name']
        print(f"\nåœºæ™¯: {description}")
        print(f"  å®¹å™¨: {container_name}")
        
        decision = analyze_with_llm(evidence)
        
        actual_command = decision['command']
        match = "âœ…" if actual_command == expected_command else "âš ï¸"
        
        print(f"  {match} é¢„æœŸ: {expected_command}, å®é™…: {actual_command}")
        print(f"  åŸå› : {decision['reason'][:60]}...")
        
        results.append({
            "container": container_name,
            "expected": expected_command,
            "actual": actual_command,
            "match": actual_command == expected_command,
            "decision": decision
        })
        
        time.sleep(1)  # é¿å… API é™æµ
    
    # ç»Ÿè®¡
    total = len(results)
    passed = sum(1 for r in results if r['match'])
    
    print(f"\n{'='*80}")
    print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
    
    # ä¿å­˜ç»“æœ
    result_file = Path(__file__).parent.parent / "logs" / "test_agent_scenarios.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": datetime.now().isoformat(),
            "total": total,
            "passed": passed,
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜: {result_file}")
    
    return passed == total


def test_online_full_diagnosis():
    """æµ‹è¯•å®Œæ•´çš„è¯Šæ–­æµç¨‹ï¼ˆå†³ç­– + æ‰§è¡Œ + é€šçŸ¥ï¼‰"""
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šå®Œæ•´è¯Šæ–­æµç¨‹ï¼ˆåœ¨çº¿ï¼‰")
    print("="*80)
    
    # æ£€æŸ¥ API Key
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âš ï¸  è·³è¿‡ï¼šæœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        return False
    
    init_config()
    
    # ä½¿ç”¨æ­£å¸¸å®¹å™¨ï¼Œä¸è§¦å‘å®é™…æ‰§è¡Œ
    evidence = get_mock_evidence_normal()
    
    print(f"è¿è¡Œå®Œæ•´è¯Šæ–­: {evidence['container']['name']}")
    
    # åŒæ­¥æ¨¡å¼ï¼ˆç­‰å¾…ç»“æœï¼‰
    result = run_diagnosis(evidence, async_mode=False)
    
    assert result is not None
    assert 'decision' in result
    assert 'timestamp' in result
    
    decision = result['decision']
    
    print(f"âœ… è¯Šæ–­å®Œæˆ")
    print(f"   - å†³ç­–: {decision['command']}")
    print(f"   - åŸå› : {decision['reason'][:60]}...")
    
    if result.get('action_result'):
        print(f"   - æ‰§è¡Œç»“æœ: {result['action_result'].get('success', 'N/A')}")
    
    if result.get('notification'):
        print(f"   - é€šçŸ¥ç»“æœ: {result['notification'].get('success', 'N/A')}")
    
    # ä¿å­˜ç»“æœ
    result_file = Path(__file__).parent.parent / "logs" / "test_agent_full_diagnosis.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": datetime.now().isoformat(),
            "evidence": evidence,
            "result": result
        }, f, ensure_ascii=False, indent=2)
    
    print(f"   - ç»“æœå·²ä¿å­˜: {result_file}")
    
    return True


# ============================================
# æµ‹è¯•è¿è¡Œå™¨
# ============================================

def run_tests(offline=True, online=True):
    """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
    print("\n")
    print("â•”" + "="*78 + "â•—")
    print("â•‘" + " "*25 + "Agent æµ‹è¯•å¥—ä»¶" + " "*35 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    results = {
        "offline": [],
        "online": []
    }
    
    # ç¦»çº¿æµ‹è¯•
    if offline:
        print("\nã€ç¦»çº¿æµ‹è¯•ã€‘ä¸éœ€è¦ API Key")
        
        offline_tests = [
            ("SYSTEM_PROMPT ç”Ÿæˆ", test_offline_prompt_generation),
            ("å†³ç­–æ ¼å¼éªŒè¯", test_offline_decision_validation),
            ("é”™è¯¯å¤„ç†", test_offline_error_handling),
        ]
        
        for name, test_func in offline_tests:
            try:
                success = test_func()
                results["offline"].append((name, success))
            except Exception as e:
                print(f"âŒ {name} å¤±è´¥: {e}")
                results["offline"].append((name, False))
    
    # åœ¨çº¿æµ‹è¯•
    if online:
        print("\nã€åœ¨çº¿æµ‹è¯•ã€‘éœ€è¦è®¾ç½® DEEPSEEK_API_KEY")
        
        online_tests = [
            ("DeepSeek API è°ƒç”¨", test_online_llm_call),
            ("å¤šåœºæ™¯å†³ç­–", test_online_multiple_scenarios),
            ("å®Œæ•´è¯Šæ–­æµç¨‹", test_online_full_diagnosis),
        ]
        
        for name, test_func in online_tests:
            try:
                success = test_func()
                results["online"].append((name, success))
            except Exception as e:
                print(f"âŒ {name} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results["online"].append((name, False))
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    if offline:
        offline_total = len(results["offline"])
        offline_passed = sum(1 for _, success in results["offline"] if success)
        print(f"\nç¦»çº¿æµ‹è¯•: {offline_passed}/{offline_total} é€šè¿‡")
        for name, success in results["offline"]:
            icon = "âœ…" if success else "âŒ"
            print(f"  {icon} {name}")
    
    if online:
        online_total = len(results["online"])
        online_passed = sum(1 for _, success in results["online"] if success)
        online_skipped = sum(1 for _, success in results["online"] if success is False)
        print(f"\nåœ¨çº¿æµ‹è¯•: {online_passed}/{online_total} é€šè¿‡")
        for name, success in results["online"]:
            icon = "âœ…" if success else ("âš ï¸" if success is False else "âŒ")
            print(f"  {icon} {name}")
    
    print("\n" + "="*80)
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Agent æµ‹è¯•å¥—ä»¶')
    parser.add_argument('--offline', action='store_true', help='ä»…è¿è¡Œç¦»çº¿æµ‹è¯•')
    parser.add_argument('--online', action='store_true', help='ä»…è¿è¡Œåœ¨çº¿æµ‹è¯•')
    
    args = parser.parse_args()
    
    # é»˜è®¤è¿è¡Œå…¨éƒ¨æµ‹è¯•
    offline = True
    online = True
    
    if args.offline and not args.online:
        online = False
    elif args.online and not args.offline:
        offline = False
    
    results = run_tests(offline=offline, online=online)
    
    # è¿”å›é€€å‡ºç 
    all_passed = all(success for _, success in results.get("offline", []) + results.get("online", []) if success is not False)
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
